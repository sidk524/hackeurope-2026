{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad85be7d",
   "metadata": {},
   "source": [
    "# Buggy CNN — MNIST Classification with Intentional Issues\n",
    "\n",
    "This notebook implements a CNN with **intentional problems** to test the ML diagnostics system:\n",
    "\n",
    "- **Over-parameterized layers**: Unnecessarily large hidden layers\n",
    "- **Redundant/duplicate layers**: Consecutive layers producing correlated outputs\n",
    "- **Vanishing gradients**: Deep network (8 conv layers) without skip connections\n",
    "- **Dead neurons**: Some layers initialized poorly, prone to dying ReLU\n",
    "- **Longer runs**: 20 epochs to trigger early stopping/diminishing returns detection\n",
    "- **Overfitting**: No dropout or regularization\n",
    "- **Memory inefficiency**: Keeping unnecessary tensors around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec9f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# observer.py lives in the parent directory (neural_network/)\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"..\")))\n",
    "from observer import Observer, ObserverConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4af738d",
   "metadata": {},
   "source": [
    "## Configuration & Hyperparameters\n",
    "\n",
    "Intentionally problematic settings:\n",
    "- **20 epochs**: Way more than needed for MNIST (should converge by ~5)\n",
    "- **High learning rate**: 0.01 can cause instability\n",
    "- **No weight decay**: Encourages overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7de8cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 20  # BUG: Way too many epochs for MNIST\n",
    "lr = 0.01  # BUG: High learning rate can cause instability\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bef515",
   "metadata": {},
   "source": [
    "## Observer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54253ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "observer_config = ObserverConfig(\n",
    "    track_profiler=True,\n",
    "    profile_every_n_steps=100,\n",
    "    track_memory=True,\n",
    "    track_throughput=True,\n",
    "    track_loss=True,\n",
    "    track_console_logs=True,\n",
    "    track_error_logs=True,\n",
    "    track_hyperparameters=True,\n",
    "    track_system_resources=True,\n",
    "    track_layer_graph=True,\n",
    "    track_layer_health=True,       # Important for detecting dead neurons, vanishing gradients\n",
    "    track_sustainability=True,     # Required for redundant layer detection\n",
    "    track_carbon_emissions=True,   # Track energy/CO2\n",
    ")\n",
    "\n",
    "observer = Observer(\n",
    "    project_id=\"1\",\n",
    "    run_name=\"buggy-cnn-mnist\",\n",
    "    config=observer_config,\n",
    ")\n",
    "\n",
    "observer.log_hyperparameters({\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"learning_rate\": lr,\n",
    "    \"optimizer\": \"SGD\",  # BUG: SGD without momentum is slower\n",
    "    \"weight_decay\": 0,   # BUG: No regularization\n",
    "    \"dataset\": \"MNIST\",\n",
    "    \"seed\": seed,\n",
    "    \"device\": device,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba37936a",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bba8e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\"data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(\"data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset):,}\")\n",
    "print(f\"Test samples:     {len(test_dataset):,}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e9b6f1",
   "metadata": {},
   "source": [
    "## Model Definition — Intentionally Buggy\n",
    "\n",
    "Issues embedded in this architecture:\n",
    "\n",
    "1. **8 conv layers deep** — prone to vanishing gradients without skip connections\n",
    "2. **Redundant conv pairs** — conv3 & conv4 have same config, same for conv5 & conv6\n",
    "3. **Over-parameterized FC layer** — 2048 neurons is overkill for MNIST\n",
    "4. **Duplicate FC layers** — fc2 & fc3 are redundant (same size)\n",
    "5. **No dropout** — encourages overfitting\n",
    "6. **Bad initialization on some layers** — constant initialization causes dead neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cfcb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuggyCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Intentionally problematic CNN for testing diagnostics.\n",
    "    \n",
    "    BUGS:\n",
    "    - Too deep (8 conv layers) for MNIST\n",
    "    - Redundant identical layer pairs\n",
    "    - Over-parameterized fully connected layers\n",
    "    - Poor initialization on some layers\n",
    "    - No regularization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # BUG: Too many conv layers for MNIST (vanishing gradients)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # BUG: Redundant pair - same input/output channels, produce correlated outputs\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)  # DUPLICATE config\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        # BUG: Another redundant pair\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1)  # DUPLICATE config\n",
    "        \n",
    "        # BUG: Tiny bottleneck followed by large layer - creates gradient issues\n",
    "        self.conv7 = nn.Conv2d(128, 16, kernel_size=1)  # Squeeze to 16 channels\n",
    "        self.conv8 = nn.Conv2d(16, 256, kernel_size=3, padding=1)  # Expand to 256\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # BUG: Massively over-parameterized FC layers for MNIST\n",
    "        # After pooling: 28 -> 14 -> 7 -> 3 (roughly), so 256 * 3 * 3 = 2304\n",
    "        self.fc1 = nn.Linear(256 * 3 * 3, 2048)  # BUG: Too large\n",
    "        \n",
    "        # BUG: Redundant FC layers with same size\n",
    "        self.fc2 = nn.Linear(2048, 512)\n",
    "        self.fc3 = nn.Linear(512, 512)  # DUPLICATE - same in/out as identity\n",
    "        self.fc4 = nn.Linear(512, 512)  # DUPLICATE - another redundant layer\n",
    "        \n",
    "        self.fc_out = nn.Linear(512, 10)\n",
    "        \n",
    "        # BUG: Bad initialization - some layers get constant init (causes dead neurons)\n",
    "        self._bad_init()\n",
    "    \n",
    "    def _bad_init(self):\n",
    "        \"\"\"Intentionally bad initialization for some layers.\"\"\"\n",
    "        # Initialize conv7 with very small weights (vanishing gradients)\n",
    "        nn.init.constant_(self.conv7.weight, 0.001)\n",
    "        nn.init.zeros_(self.conv7.bias)\n",
    "        \n",
    "        # Initialize fc3 and fc4 with near-zero weights (redundant/dead layers)\n",
    "        nn.init.normal_(self.fc3.weight, mean=0, std=0.001)\n",
    "        nn.init.zeros_(self.fc3.bias)\n",
    "        nn.init.normal_(self.fc4.weight, mean=0, std=0.001)\n",
    "        nn.init.zeros_(self.fc4.bias)\n",
    "    \n",
    "    def forward(self, x, targets=None):\n",
    "        # Block 1: 28x28 -> 14x14\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        # Block 2: 14x14 -> 7x7 (redundant conv3, conv4)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(F.relu(self.conv4(x)))  # Redundant\n",
    "        \n",
    "        # Block 3: 7x7 -> 3x3 (redundant conv5, conv6)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.pool(F.relu(self.conv6(x)))  # Redundant\n",
    "        \n",
    "        # Block 4: bottleneck (causes gradient issues)\n",
    "        x = F.relu(self.conv7(x))  # Squeeze\n",
    "        x = F.relu(self.conv8(x))  # Expand\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Over-parameterized FC layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))  # Redundant\n",
    "        x = F.relu(self.fc4(x))  # Redundant\n",
    "        \n",
    "        logits = self.fc_out(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d80f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BuggyCNN().to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {num_params:,}\")\n",
    "print(f\"\\nThis is MASSIVELY over-parameterized for MNIST!\")\n",
    "print(f\"A good MNIST model needs ~50k params, this has {num_params:,}\")\n",
    "\n",
    "observer.register_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e89da56",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Using SGD without momentum (slower convergence) and running for 20 epochs (way too many)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    \"\"\"Compute average loss and accuracy on a DataLoader.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits, loss = model(x, y)\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    model.train()\n",
    "    return total_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534d0d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUG: Using SGD without momentum (slower) and no weight decay (overfitting)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# BUG: Intentionally keeping track of losses in a list that grows (memory leak pattern)\n",
    "all_losses = []  # Memory inefficiency\n",
    "\n",
    "print(f\"Starting training for {num_epochs} epochs...\")\n",
    "print(f\"WARNING: This is intentionally buggy and will run longer than necessary!\")\n",
    "training_start = time.time()\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for step, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        if observer.should_profile(global_step):\n",
    "            logits, loss = observer.profile_step(model, x, y)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        else:\n",
    "            logits, loss = model(x, y)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # BUG: Storing full loss tensor (not .item()) - memory leak\n",
    "        epoch_losses.append(loss.detach().cpu())\n",
    "        observer.step(global_step, loss, batch_size=x.size(0))\n",
    "        global_step += 1\n",
    "    \n",
    "    # BUG: Accumulating all epoch losses (memory growth)\n",
    "    all_losses.extend(epoch_losses)\n",
    "\n",
    "    # Validation at end of each epoch\n",
    "    val_loss, val_acc = evaluate(model, test_loader)\n",
    "    step_report = observer.flush(val_metrics={\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "    })\n",
    "\n",
    "    elapsed = time.time() - training_start\n",
    "    train_loss = step_report['loss']['train_mean']\n",
    "    \n",
    "    # Detect overfitting pattern\n",
    "    overfit_warning = \"\"\n",
    "    if val_loss > train_loss * 1.5:\n",
    "        overfit_warning = \" [OVERFITTING!]\"\n",
    "    \n",
    "    print(\n",
    "        f\"Epoch {epoch:2d}: \"\n",
    "        f\"train_loss={train_loss:.4f}  \"\n",
    "        f\"val_loss={val_loss:.4f}  val_acc={val_acc:.4f}  \"\n",
    "        f\"({elapsed:.1f}s){overfit_warning}\"\n",
    "    )\n",
    "\n",
    "training_time = time.time() - training_start\n",
    "print(f\"\\nTraining completed in {training_time:.2f}s ({training_time/60:.2f} min)\")\n",
    "print(f\"Memory used by accumulated losses: {len(all_losses)} tensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b008f5",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fcf0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader)\n",
    "print(f\"Final test loss:     {test_loss:.4f}\")\n",
    "print(f\"Final test accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"EXPECTED ISSUES FOR DIAGNOSTICS TO DETECT:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Diminishing returns / Early stopping opportunity\")\n",
    "print(\"2. Over-parameterized layers (fc1 with 2048 neurons)\")\n",
    "print(\"3. Redundant layers (conv3/conv4, conv5/conv6, fc3/fc4)\")\n",
    "print(\"4. Vanishing gradients (deep network, bottleneck at conv7)\")\n",
    "print(\"5. Dead/near-zero weights (fc3, fc4 bad init)\")\n",
    "print(\"6. Loss plateau (after ~5 epochs)\")\n",
    "print(\"7. Overfitting (val_loss > train_loss)\")\n",
    "print(\"8. Memory growth (accumulated losses)\")\n",
    "print(\"9. CPU-only training (sustainability warning)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ea7c26",
   "metadata": {},
   "source": [
    "## Observer Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3e502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = observer.export(os.path.join(\"observer_reports\", f\"{observer.run_id}.json\"))\n",
    "\n",
    "# ── Print summary ──\n",
    "summary = report[\"summary\"]\n",
    "print(\"=\" * 60)\n",
    "print(\"OBSERVER SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total steps recorded:   {summary.get('total_steps', 0)}\")\n",
    "print(f\"Total training time:    {summary.get('total_duration_s', 0):.2f}s\")\n",
    "\n",
    "if \"loss_trend\" in summary:\n",
    "    lt = summary[\"loss_trend\"]\n",
    "    print(f\"\\nLoss trend:\")\n",
    "    print(f\"  First interval:  {lt['first']:.4f}\")\n",
    "    print(f\"  Last interval:   {lt['last']:.4f}\")\n",
    "    print(f\"  Best:            {lt['best']:.4f}\")\n",
    "    print(f\"  Improved:        {lt['improved']}\")\n",
    "\n",
    "if \"avg_tokens_per_sec\" in summary:\n",
    "    print(f\"\\nAvg throughput:  {summary['avg_tokens_per_sec']:.0f} tokens/sec\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Full report saved to: observer_reports/{observer.run_id}.json\")\n",
    "print(f\"\\nRun diagnostics API: POST /diagnostics/sessions/{{session_id}}/run\")\n",
    "\n",
    "observer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ca173",
   "metadata": {},
   "source": [
    "## How to Test Diagnostics\n",
    "\n",
    "After running this notebook:\n",
    "\n",
    "1. **Upload the report** to the backend database\n",
    "2. **Call the diagnostics API**:\n",
    "   ```bash\n",
    "   curl -X POST http://localhost:8000/diagnostics/sessions/{session_id}/run\n",
    "   ```\n",
    "3. **Expected diagnostics findings**:\n",
    "   - `sustainability`: Early stopping opportunity, wasted compute\n",
    "   - `sustainability`: Over-parameterized layers\n",
    "   - `sustainability`: Redundant layers (correlated outputs)\n",
    "   - `sustainability`: Vanishing gradients\n",
    "   - `sustainability`: Dead neurons / near-zero weights\n",
    "   - `loss`: Plateau detection\n",
    "   - `loss`: Overfitting warning\n",
    "   - `memory`: Memory growth\n",
    "   - `system`: CPU-only training"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
