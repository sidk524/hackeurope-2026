{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini GPT Transformer\n",
    "\n",
    "A character-level GPT language model trained on text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# observer.py lives in the parent directory (neural_network/)\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"..\")))\n",
    "from observer import Observer, ObserverConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Dataset and model paths\n",
    "DATASET_NAME = \"input_childSpeech_trainingSet.txt\"\n",
    "MODEL_NAME = os.path.join(\"models\", \"model_checkpoint.pt\")\n",
    "LOAD_MODEL = False\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "block_size = 128\n",
    "max_iters = 1\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "eval_iters = 200\n",
    "n_embd = 128\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.2\n",
    "\n",
    "USE_BIAS_IN_ATTENTION = False\n",
    "USE_SKIP_CONNECTIONS = True\n",
    "\n",
    "seed = 1337\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Observer] Initialized | project=mini-gpt-experiment | run=childSpeech_128emb_4L_4H | device=cpu\n",
      "[Observer] Hyperparameters logged: ['dataset', 'batch_size', 'block_size', 'max_iters', 'eval_interval', 'learning_rate', 'n_embd', 'n_head', 'n_layer', 'dropout', 'use_bias_in_attention', 'use_skip_connections', 'seed', 'device', 'eval_iters', 'optimizer']\n"
     ]
    }
   ],
   "source": [
    "# ── Configure which telemetry channels to observe ──\n",
    "observer_config = ObserverConfig(\n",
    "    track_profiler=True,              # PyTorch profiler: op-level CPU/CUDA time, memory, shapes\n",
    "    track_gradients=True,             # Per-layer gradient norms, vanishing/exploding detection\n",
    "    track_activations=True,           # Activation mean/std/dead-neuron% per Linear/LayerNorm/Embedding\n",
    "    track_memory=True,                # Process RSS + CUDA allocated/reserved/peak\n",
    "    track_throughput=True,            # Samples/sec, tokens/sec, batches/sec\n",
    "    track_loss=True,                  # Per-epoch loss statistics (mean, min, max, std)\n",
    "    track_weights=True,               # Weight distribution evolution (norm, mean, std)\n",
    "    track_attention_entropy=True,     # Attention output entropy & sparsity (transformer-specific)\n",
    "    track_console_logs=True,          # Capture INFO-level console output\n",
    "    track_error_logs=True,            # Capture WARNING+ error logs\n",
    "    track_hyperparameters=True,       # Record all hyperparameters\n",
    "    track_system_resources=True,      # CPU %, RAM usage, GPU info\n",
    ")\n",
    "\n",
    "# ── Initialize Observer ──\n",
    "observer = Observer(\n",
    "    api_key=\"your-api-key-here\",\n",
    "    project_id=\"mini-gpt-experiment\",\n",
    "    config=observer_config,\n",
    "    run_name=f\"childSpeech_{n_embd}emb_{n_layer}L_{n_head}H\",\n",
    ")\n",
    "\n",
    "# ── Log hyperparameters ──\n",
    "observer.log_hyperparameters({\n",
    "    \"dataset\": DATASET_NAME,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"block_size\": block_size,\n",
    "    \"max_iters\": max_iters,\n",
    "    \"eval_interval\": eval_interval,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"n_embd\": n_embd,\n",
    "    \"n_head\": n_head,\n",
    "    \"n_layer\": n_layer,\n",
    "    \"dropout\": dropout,\n",
    "    \"use_bias_in_attention\": USE_BIAS_IN_ATTENTION,\n",
    "    \"use_skip_connections\": USE_SKIP_CONNECTIONS,\n",
    "    \"seed\": seed,\n",
    "    \"device\": device,\n",
    "    \"eval_iters\": eval_iters,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Logging & Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "model_base_name = os.path.splitext(os.path.basename(MODEL_NAME))[0]\n",
    "log_filename = os.path.join(\"logs\", f\"{model_base_name}_{DATASET_NAME}.log\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename, mode=\"w\", encoding=\"utf-8\"),\n",
    "        logging.StreamHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(f\"Log file: {log_filename}\\n\")\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Loading dataset: {DATASET_NAME}\")\n",
    "\n",
    "with open(DATASET_NAME, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "logger.info(f\"Total characters in dataset: {len(text):,}\")\n",
    "logger.info(f\"First 200 characters preview:\\n{text[:200]}\\n\")\n",
    "\n",
    "# Unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "logger.info(f\"Vocabulary size: {vocab_size}\")\n",
    "logger.info(f\"Unique characters: {''.join(chars)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character frequencies\n",
    "char_counts = Counter(text)\n",
    "logger.info(\"Top 5 Character frequencies:\")\n",
    "for char, count in char_counts.most_common(5):\n",
    "    logger.info(f\"'{char}': {count}\")\n",
    "\n",
    "# Most used words\n",
    "words = text.split()\n",
    "word_counts = Counter(words)\n",
    "logger.info(\"Most common 5 words:\")\n",
    "for word, count in word_counts.most_common(5):\n",
    "    logger.info(f\"'{word}': {count}\")\n",
    "\n",
    "unique_words = len(word_counts)\n",
    "logger.info(f\"Total unique words in dataset: {unique_words}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding & Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character-to-integer mapping\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "# Train/val split\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "logger.info(f\"Training set size: {len(train_data):,} characters\")\n",
    "logger.info(f\"Validation set size: {len(val_data):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optionally Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = None\n",
    "if LOAD_MODEL:\n",
    "    logger.info(f\"\\nLoading existing model from: {MODEL_NAME}\")\n",
    "    try:\n",
    "        checkpoint = torch.load(MODEL_NAME, map_location=device)\n",
    "\n",
    "        vocab_size = checkpoint[\"vocab_size\"]\n",
    "        n_embd = checkpoint[\"n_embd\"]\n",
    "        n_head = checkpoint[\"n_head\"]\n",
    "        n_layer = checkpoint[\"n_layer\"]\n",
    "        block_size = checkpoint[\"block_size\"]\n",
    "        dropout = checkpoint[\"dropout\"]\n",
    "        USE_BIAS_IN_ATTENTION = checkpoint[\"use_bias\"]\n",
    "        USE_SKIP_CONNECTIONS = checkpoint[\"use_skip\"]\n",
    "        stoi = checkpoint[\"stoi\"]\n",
    "        itos = checkpoint[\"itos\"]\n",
    "\n",
    "        logger.info(f\"Loaded model configuration:\")\n",
    "        logger.info(f\"  Vocabulary size: {vocab_size}\")\n",
    "        logger.info(f\"  Embedding dimension: {n_embd}\")\n",
    "        logger.info(f\"  Attention heads: {n_head}\")\n",
    "        logger.info(f\"  Layers: {n_layer}\")\n",
    "        logger.info(f\"  Block size: {block_size}\")\n",
    "        logger.info(f\"  Previous train loss: {checkpoint['train_loss']:.4f}\")\n",
    "        logger.info(f\"  Previous val loss: {checkpoint['val_loss']:.4f}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logger.info(f\"Error: Model file '{MODEL_NAME}' not found!\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error loading model: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    \"\"\"Generate a small batch of data of inputs x and targets y.\"\"\"\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention.\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=USE_BIAS_IN_ATTENTION)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=USE_BIAS_IN_ATTENTION)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=USE_BIAS_IN_ATTENTION)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)    # (B,T,hs)\n",
    "        q = self.query(x)  # (B,T,hs)\n",
    "        # Compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5  # (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        # Weighted aggregation of values\n",
    "        v = self.value(x)  # (B,T,hs)\n",
    "        out = wei @ v      # (B, T, hs)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\"A simple linear layer followed by a non-linearity.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if USE_SKIP_CONNECTIONS:\n",
    "            x = x + self.sa(self.ln1(x))\n",
    "            x = x + self.ffwd(self.ln2(x))\n",
    "        else:\n",
    "            x = self.sa(self.ln1(x))\n",
    "            x = self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)                          # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        x = self.blocks(x)     # (B,T,C)\n",
    "        x = self.ln_f(x)       # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]       # (B, C)\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Observer] Model registered | 818,472 params (0.82M) | 72 param layers\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Model Configuration:\")\n",
    "logger.info(f\"Embedding dimension (n_embd): {n_embd}\")\n",
    "logger.info(f\"Number of attention heads (n_head): {n_head}\")\n",
    "logger.info(f\"Number of layers (n_layer): {n_layer}\")\n",
    "logger.info(f\"Block size (context length): {block_size}\")\n",
    "logger.info(f\"Batch size: {batch_size}\")\n",
    "logger.info(f\"Dropout: {dropout}\")\n",
    "logger.info(f\"Use bias in attention: {USE_BIAS_IN_ATTENTION}\")\n",
    "logger.info(f\"Use skip connections: {USE_SKIP_CONNECTIONS}\")\n",
    "logger.info(f\"Device: {device}\")\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "if LOAD_MODEL and checkpoint is not None:\n",
    "    m.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    logger.info(\"Model weights loaded successfully!\")\n",
    "\n",
    "num_params = sum(p.numel() for p in m.parameters())\n",
    "logger.info(f\"\\nTotal parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\n",
    "\n",
    "# Register model with the observer (attaches activation & attention hooks,\n",
    "# captures architecture map with per-layer param counts)\n",
    "observer.register_model(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Observer] --- Epoch 0 started ---\n",
      "c:\\Users\\admin\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\profiler\\profiler.py:217: UserWarning: Warning: Profiler clears events at the end of each cycle.Only events from the current cycle will be reported.To keep events across cycles, set acc_events=True.\n",
      "  _warn_once(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m     global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Evaluate at end of epoch\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m training_start_time\n\u001b[0;32m     49\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglobal_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_iters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(elapsed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\utils\\_contextlib.py:124\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[9], line 19\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[0;32m     18\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(split)\n\u001b[1;32m---> 19\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     21\u001b[0m out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\admin\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1786\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[10], line 105\u001b[0m, in \u001b[0;36mGPTLanguageModel.forward\u001b[1;34m(self, idx, targets)\u001b[0m\n\u001b[0;32m    103\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice))  \u001b[38;5;66;03m# (T,C)\u001b[39;00m\n\u001b[0;32m    104\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb  \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m     \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[0;32m    106\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x)       \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[0;32m    107\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)  \u001b[38;5;66;03m# (B,T,vocab_size)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1786\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\nn\\modules\\container.py:253\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03mRuns the forward pass.\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\admin\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1786\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[10], line 72\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m USE_SKIP_CONNECTIONS:\n\u001b[0;32m     71\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x))\n\u001b[1;32m---> 72\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x))\n",
      "File \u001b[1;32mc:\\Users\\admin\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1786\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[10], line 55\u001b[0m, in \u001b[0;36mFeedFoward.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1786\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\nn\\modules\\container.py:253\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03mRuns the forward pass.\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\admin\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1882\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1881\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1882\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1884\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[0;32m   1885\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[0;32m   1886\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[0;32m   1887\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\admin\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1843\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1841\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[0;32m   1842\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1843\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1846\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[1;32mf:\\hackeurope-2026\\neural_network\\sample\\observer.py:315\u001b[0m, in \u001b[0;36mhook\u001b[1;34m(_module, _input, output)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    logger.info(\"\\nSkipping training (LOAD_MODEL is set)\")\n",
    "    logger.info(\"Proceeding directly to evaluation...\\n\")\n",
    "    losses = (\n",
    "        {\"train\": checkpoint[\"train_loss\"], \"val\": checkpoint[\"val_loss\"]}\n",
    "        if checkpoint\n",
    "        else {}\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"Starting training...\")\n",
    "    training_start_time = time.time()\n",
    "\n",
    "    # ── Epoch-based training with Observer integration ──\n",
    "    # Each epoch = eval_interval steps (or remaining steps for the last epoch)\n",
    "    steps_per_epoch = min(eval_interval, max_iters) if eval_interval > 0 else max_iters\n",
    "    num_epochs = max(1, (max_iters + steps_per_epoch - 1) // steps_per_epoch)\n",
    "    global_step = 0\n",
    "    losses = {}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        observer.start_epoch(epoch)\n",
    "        epoch_steps = min(steps_per_epoch, max_iters - global_step)\n",
    "\n",
    "        for step in range(epoch_steps):\n",
    "            xb, yb = get_batch(\"train\")\n",
    "\n",
    "            # On the first batch of each epoch, run a profiled step\n",
    "            # This captures op-level CPU/CUDA time, memory, forward/backward breakdown\n",
    "            if step == 0 and observer.config.track_profiler:\n",
    "                logits, loss = observer.profile_step(model, xb, yb)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            else:\n",
    "                logits, loss = model(xb, yb)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            observer.log_batch(\n",
    "                step, loss, batch_size=batch_size, seq_length=block_size\n",
    "            )\n",
    "            global_step += 1\n",
    "\n",
    "        # Evaluate at end of epoch\n",
    "        losses = estimate_loss()\n",
    "        elapsed = time.time() - training_start_time\n",
    "        logger.info(\n",
    "            f\"Epoch {epoch} (step {global_step}/{max_iters}): \"\n",
    "            f\"train loss {losses['train']:.4f}, val loss {losses['val']:.4f} \"\n",
    "            f\"(elapsed: {elapsed:.1f}s)\"\n",
    "        )\n",
    "\n",
    "        # Finalize epoch — collects gradients, weights, activations, memory,\n",
    "        # throughput, profiler data, attention entropy, system stats\n",
    "        epoch_report = observer.end_epoch(epoch, val_metrics={\n",
    "            \"val_loss\": losses[\"val\"],\n",
    "            \"train_loss_eval\": losses[\"train\"],\n",
    "        })\n",
    "\n",
    "    training_end_time = time.time()\n",
    "    training_time = training_end_time - training_start_time\n",
    "    logger.info(\n",
    "        f\"Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)!\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_MODEL:\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"vocab_size\": vocab_size,\n",
    "            \"n_embd\": n_embd,\n",
    "            \"n_head\": n_head,\n",
    "            \"n_layer\": n_layer,\n",
    "            \"block_size\": block_size,\n",
    "            \"dropout\": dropout,\n",
    "            \"use_bias\": USE_BIAS_IN_ATTENTION,\n",
    "            \"use_skip\": USE_SKIP_CONNECTIONS,\n",
    "            \"stoi\": stoi,\n",
    "            \"itos\": itos,\n",
    "            \"train_loss\": losses[\"train\"].item(),\n",
    "            \"val_loss\": losses[\"val\"].item(),\n",
    "        },\n",
    "        MODEL_NAME,\n",
    "    )\n",
    "    logger.info(f\"Model saved to {MODEL_NAME}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observer Report\n",
    "\n",
    "Export all collected epoch-by-epoch data: profiler ops, gradient health, activation stats,\n",
    "weight evolution, memory footprint, throughput, attention entropy, and system resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export full observer report to JSON\n",
    "report = observer.export(os.path.join(\"observer_reports\", f\"{observer.run_id}.json\"))\n",
    "\n",
    "# ── Print summary ──\n",
    "summary = report[\"summary\"]\n",
    "print(\"=\" * 60)\n",
    "print(\"OBSERVER SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total epochs recorded:  {summary.get('total_epochs', 0)}\")\n",
    "print(f\"Total training time:    {summary.get('total_duration_s', 0):.2f}s\")\n",
    "\n",
    "if \"loss_trend\" in summary:\n",
    "    lt = summary[\"loss_trend\"]\n",
    "    print(f\"\\nLoss trend:\")\n",
    "    print(f\"  First epoch:  {lt['first']:.4f}\")\n",
    "    print(f\"  Last epoch:   {lt['last']:.4f}\")\n",
    "    print(f\"  Best:         {lt['best']:.4f}\")\n",
    "    print(f\"  Improved:     {lt['improved']}\")\n",
    "\n",
    "if \"gradient_health\" in summary:\n",
    "    gh = summary[\"gradient_health\"]\n",
    "    print(f\"\\nGradient health:\")\n",
    "    print(f\"  Epochs with issues:  {gh['epochs_with_issues']}\")\n",
    "    print(f\"  Total issues:        {gh['total_issues']}\")\n",
    "\n",
    "if \"avg_tokens_per_sec\" in summary:\n",
    "    print(f\"\\nAvg throughput:  {summary['avg_tokens_per_sec']:.0f} tokens/sec\")\n",
    "\n",
    "if \"profiler_highlight\" in summary:\n",
    "    ph = summary[\"profiler_highlight\"]\n",
    "    print(f\"\\nProfiler highlight:\")\n",
    "    print(f\"  Top operation:       {ph.get('top_op', 'N/A')}\")\n",
    "    print(f\"  Top op % of total:   {ph.get('top_op_pct', 0):.1f}%\")\n",
    "    print(f\"  Fwd/Bwd time ratio:  {ph.get('fwd_bwd_ratio', 'N/A')}\")\n",
    "\n",
    "# ── Print per-epoch profiler categories ──\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PROFILER: OPERATION CATEGORIES (last epoch)\")\n",
    "print(\"=\" * 60)\n",
    "for epoch_rec in reversed(report[\"epochs\"]):\n",
    "    if \"profiler\" in epoch_rec:\n",
    "        cats = epoch_rec[\"profiler\"].get(\"operation_categories\", {})\n",
    "        for cat_name, cat_data in sorted(cats.items(), key=lambda x: -x[1][\"cpu_time_ms\"]):\n",
    "            print(f\"  {cat_name:<20s}  {cat_data['cpu_time_ms']:>8.1f}ms  ({cat_data['pct_cpu']:>5.1f}%)\")\n",
    "        break\n",
    "\n",
    "# ── Print per-epoch gradient norms ──\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GRADIENT TOTAL NORMS (per epoch)\")\n",
    "print(\"=\" * 60)\n",
    "for epoch_rec in report[\"epochs\"]:\n",
    "    if \"gradients\" in epoch_rec:\n",
    "        g = epoch_rec[\"gradients\"]\n",
    "        health = g.get(\"health\", \"?\")\n",
    "        print(f\"  Epoch {epoch_rec['epoch']:>3d}:  norm={g['total_norm']:.4f}  [{health}]\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Full report saved to: observer_reports/{observer.run_id}.json\")\n",
    "\n",
    "# Clean up observer hooks\n",
    "observer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Generated text sample:\")\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "logger.info(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_on_test_file(\n",
    "    model, filename, stoi, block_size, device, batch_size, eval_iters=200\n",
    "):\n",
    "    \"\"\"\n",
    "    Load a test file and calculate the loss.\n",
    "    Handles characters not in vocabulary by skipping them.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"\\nEvaluating on: {filename}\")\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            test_text = f.read()\n",
    "\n",
    "        logger.info(f\"Test file length: {len(test_text):,} characters\")\n",
    "\n",
    "        test_encoded = []\n",
    "        skipped_chars = set()\n",
    "        for c in test_text:\n",
    "            if c in stoi:\n",
    "                test_encoded.append(stoi[c])\n",
    "            else:\n",
    "                skipped_chars.add(c)\n",
    "\n",
    "        if skipped_chars:\n",
    "            logger.info(\n",
    "                f\"Warning: Skipped {len(skipped_chars)} characters not in vocabulary: {skipped_chars}\"\n",
    "            )\n",
    "\n",
    "        test_data = torch.tensor(test_encoded, dtype=torch.long)\n",
    "        logger.info(f\"Encoded test data length: {len(test_data):,} tokens\")\n",
    "\n",
    "        if len(test_data) <= block_size:\n",
    "            logger.info(\"Warning: Test data too short for evaluation\")\n",
    "            return None\n",
    "\n",
    "        model.eval()\n",
    "        losses = []\n",
    "\n",
    "        num_batches = min(eval_iters, len(test_data) // block_size)\n",
    "        for _ in range(num_batches):\n",
    "            ix = torch.randint(len(test_data) - block_size, (batch_size,))\n",
    "            x = torch.stack([test_data[i : i + block_size] for i in ix])\n",
    "            y = torch.stack([test_data[i + 1 : i + block_size + 1] for i in ix])\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            logits, loss = model(x, y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        if losses:\n",
    "            avg_loss = sum(losses) / len(losses)\n",
    "            logger.info(f\"Average loss on {filename}: {avg_loss:.4f}\")\n",
    "            return avg_loss\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logger.info(f\"Error: File {filename} not found\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error evaluating on {filename}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Evaluating on Test Sets\")\n",
    "\n",
    "test_loss_child = evaluate_on_test_file(\n",
    "    model, \"input_childSpeech_testSet.txt\", stoi, block_size, device, batch_size, eval_iters=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_baseline_loss(text, stoi):\n",
    "    \"\"\"\n",
    "    Calculate baseline loss using uniform character distribution.\n",
    "    This is the loss if the model just guessed randomly.\n",
    "    \"\"\"\n",
    "    char_counts = {}\n",
    "    total_chars = 0\n",
    "    for c in text:\n",
    "        if c in stoi:\n",
    "            char_counts[c] = char_counts.get(c, 0) + 1\n",
    "            total_chars += 1\n",
    "\n",
    "    entropy = 0\n",
    "    for count in char_counts.values():\n",
    "        prob = count / total_chars\n",
    "        entropy -= prob * math.log(prob)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "\n",
    "logger.info(\"Baseline Comparison:\")\n",
    "baseline_train = calculate_baseline_loss(text, stoi)\n",
    "logger.info(f\"Baseline loss (uniform distribution): {baseline_train:.4f}\")\n",
    "logger.info(f\"Training loss: {losses['train']:.4f}\")\n",
    "logger.info(f\"Validation loss: {losses['val']:.4f}\")\n",
    "if test_loss_child:\n",
    "    logger.info(f\"Test loss (Child Speech): {test_loss_child:.4f}\")\n",
    "if test_loss_shakespeare:\n",
    "    logger.info(f\"Test loss (Shakespeare): {test_loss_shakespeare:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "logger.info(\n",
    "    f\"Total execution time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
