{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini GPT Transformer\n",
    "\n",
    "A character-level GPT language model trained on text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and model paths\n",
    "DATASET_NAME = \"input_childSpeech_trainingSet.txt\"\n",
    "MODEL_NAME = os.path.join(\"models\", \"model_checkpoint.pt\")\n",
    "LOAD_MODEL = False\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "block_size = 128\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "eval_iters = 200\n",
    "n_embd = 128\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.2\n",
    "\n",
    "USE_BIAS_IN_ATTENTION = False\n",
    "USE_SKIP_CONNECTIONS = True\n",
    "\n",
    "seed = 1337\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Logging & Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "model_base_name = os.path.splitext(os.path.basename(MODEL_NAME))[0]\n",
    "log_filename = os.path.join(\"logs\", f\"{model_base_name}_{DATASET_NAME}.log\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename, mode=\"w\", encoding=\"utf-8\"),\n",
    "        logging.StreamHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(f\"Log file: {log_filename}\\n\")\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Loading dataset: {DATASET_NAME}\")\n",
    "\n",
    "with open(DATASET_NAME, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "logger.info(f\"Total characters in dataset: {len(text):,}\")\n",
    "logger.info(f\"First 200 characters preview:\\n{text[:200]}\\n\")\n",
    "\n",
    "# Unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "logger.info(f\"Vocabulary size: {vocab_size}\")\n",
    "logger.info(f\"Unique characters: {''.join(chars)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character frequencies\n",
    "char_counts = Counter(text)\n",
    "logger.info(\"Top 5 Character frequencies:\")\n",
    "for char, count in char_counts.most_common(5):\n",
    "    logger.info(f\"'{char}': {count}\")\n",
    "\n",
    "# Most used words\n",
    "words = text.split()\n",
    "word_counts = Counter(words)\n",
    "logger.info(\"Most common 5 words:\")\n",
    "for word, count in word_counts.most_common(5):\n",
    "    logger.info(f\"'{word}': {count}\")\n",
    "\n",
    "unique_words = len(word_counts)\n",
    "logger.info(f\"Total unique words in dataset: {unique_words}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding & Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character-to-integer mapping\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "# Train/val split\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "logger.info(f\"Training set size: {len(train_data):,} characters\")\n",
    "logger.info(f\"Validation set size: {len(val_data):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optionally Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = None\n",
    "if LOAD_MODEL:\n",
    "    logger.info(f\"\\nLoading existing model from: {MODEL_NAME}\")\n",
    "    try:\n",
    "        checkpoint = torch.load(MODEL_NAME, map_location=device)\n",
    "\n",
    "        vocab_size = checkpoint[\"vocab_size\"]\n",
    "        n_embd = checkpoint[\"n_embd\"]\n",
    "        n_head = checkpoint[\"n_head\"]\n",
    "        n_layer = checkpoint[\"n_layer\"]\n",
    "        block_size = checkpoint[\"block_size\"]\n",
    "        dropout = checkpoint[\"dropout\"]\n",
    "        USE_BIAS_IN_ATTENTION = checkpoint[\"use_bias\"]\n",
    "        USE_SKIP_CONNECTIONS = checkpoint[\"use_skip\"]\n",
    "        stoi = checkpoint[\"stoi\"]\n",
    "        itos = checkpoint[\"itos\"]\n",
    "\n",
    "        logger.info(f\"Loaded model configuration:\")\n",
    "        logger.info(f\"  Vocabulary size: {vocab_size}\")\n",
    "        logger.info(f\"  Embedding dimension: {n_embd}\")\n",
    "        logger.info(f\"  Attention heads: {n_head}\")\n",
    "        logger.info(f\"  Layers: {n_layer}\")\n",
    "        logger.info(f\"  Block size: {block_size}\")\n",
    "        logger.info(f\"  Previous train loss: {checkpoint['train_loss']:.4f}\")\n",
    "        logger.info(f\"  Previous val loss: {checkpoint['val_loss']:.4f}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logger.info(f\"Error: Model file '{MODEL_NAME}' not found!\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error loading model: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    \"\"\"Generate a small batch of data of inputs x and targets y.\"\"\"\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention.\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=USE_BIAS_IN_ATTENTION)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=USE_BIAS_IN_ATTENTION)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=USE_BIAS_IN_ATTENTION)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)    # (B,T,hs)\n",
    "        q = self.query(x)  # (B,T,hs)\n",
    "        # Compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5  # (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        # Weighted aggregation of values\n",
    "        v = self.value(x)  # (B,T,hs)\n",
    "        out = wei @ v      # (B, T, hs)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\"A simple linear layer followed by a non-linearity.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if USE_SKIP_CONNECTIONS:\n",
    "            x = x + self.sa(self.ln1(x))\n",
    "            x = x + self.ffwd(self.ln2(x))\n",
    "        else:\n",
    "            x = self.sa(self.ln1(x))\n",
    "            x = self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)                          # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        x = self.blocks(x)     # (B,T,C)\n",
    "        x = self.ln_f(x)       # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]       # (B, C)\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Model Configuration:\")\n",
    "logger.info(f\"Embedding dimension (n_embd): {n_embd}\")\n",
    "logger.info(f\"Number of attention heads (n_head): {n_head}\")\n",
    "logger.info(f\"Number of layers (n_layer): {n_layer}\")\n",
    "logger.info(f\"Block size (context length): {block_size}\")\n",
    "logger.info(f\"Batch size: {batch_size}\")\n",
    "logger.info(f\"Dropout: {dropout}\")\n",
    "logger.info(f\"Use bias in attention: {USE_BIAS_IN_ATTENTION}\")\n",
    "logger.info(f\"Use skip connections: {USE_SKIP_CONNECTIONS}\")\n",
    "logger.info(f\"Device: {device}\")\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "if LOAD_MODEL and checkpoint is not None:\n",
    "    m.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    logger.info(\"Model weights loaded successfully!\")\n",
    "\n",
    "num_params = sum(p.numel() for p in m.parameters())\n",
    "logger.info(f\"\\nTotal parameters: {num_params:,} ({num_params/1e6:.2f}M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    logger.info(\"\\nSkipping training (LOAD_MODEL is set)\")\n",
    "    logger.info(\"Proceeding directly to evaluation...\\n\")\n",
    "    losses = (\n",
    "        {\"train\": checkpoint[\"train_loss\"], \"val\": checkpoint[\"val_loss\"]}\n",
    "        if checkpoint\n",
    "        else {}\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"Starting training...\")\n",
    "    training_start_time = time.time()\n",
    "\n",
    "    losses = {}\n",
    "    for iter in range(max_iters):\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss()\n",
    "            elapsed = time.time() - training_start_time\n",
    "            logger.info(\n",
    "                f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f} \"\n",
    "                f\"(elapsed: {elapsed:.1f}s)\"\n",
    "            )\n",
    "\n",
    "        xb, yb = get_batch(\"train\")\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    training_end_time = time.time()\n",
    "    training_time = training_end_time - training_start_time\n",
    "    logger.info(\n",
    "        f\"Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)!\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_MODEL:\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"vocab_size\": vocab_size,\n",
    "            \"n_embd\": n_embd,\n",
    "            \"n_head\": n_head,\n",
    "            \"n_layer\": n_layer,\n",
    "            \"block_size\": block_size,\n",
    "            \"dropout\": dropout,\n",
    "            \"use_bias\": USE_BIAS_IN_ATTENTION,\n",
    "            \"use_skip\": USE_SKIP_CONNECTIONS,\n",
    "            \"stoi\": stoi,\n",
    "            \"itos\": itos,\n",
    "            \"train_loss\": losses[\"train\"].item(),\n",
    "            \"val_loss\": losses[\"val\"].item(),\n",
    "        },\n",
    "        MODEL_NAME,\n",
    "    )\n",
    "    logger.info(f\"Model saved to {MODEL_NAME}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Generated text sample:\")\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "logger.info(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_on_test_file(\n",
    "    model, filename, stoi, block_size, device, batch_size, eval_iters=200\n",
    "):\n",
    "    \"\"\"\n",
    "    Load a test file and calculate the loss.\n",
    "    Handles characters not in vocabulary by skipping them.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"\\nEvaluating on: {filename}\")\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            test_text = f.read()\n",
    "\n",
    "        logger.info(f\"Test file length: {len(test_text):,} characters\")\n",
    "\n",
    "        test_encoded = []\n",
    "        skipped_chars = set()\n",
    "        for c in test_text:\n",
    "            if c in stoi:\n",
    "                test_encoded.append(stoi[c])\n",
    "            else:\n",
    "                skipped_chars.add(c)\n",
    "\n",
    "        if skipped_chars:\n",
    "            logger.info(\n",
    "                f\"Warning: Skipped {len(skipped_chars)} characters not in vocabulary: {skipped_chars}\"\n",
    "            )\n",
    "\n",
    "        test_data = torch.tensor(test_encoded, dtype=torch.long)\n",
    "        logger.info(f\"Encoded test data length: {len(test_data):,} tokens\")\n",
    "\n",
    "        if len(test_data) <= block_size:\n",
    "            logger.info(\"Warning: Test data too short for evaluation\")\n",
    "            return None\n",
    "\n",
    "        model.eval()\n",
    "        losses = []\n",
    "\n",
    "        num_batches = min(eval_iters, len(test_data) // block_size)\n",
    "        for _ in range(num_batches):\n",
    "            ix = torch.randint(len(test_data) - block_size, (batch_size,))\n",
    "            x = torch.stack([test_data[i : i + block_size] for i in ix])\n",
    "            y = torch.stack([test_data[i + 1 : i + block_size + 1] for i in ix])\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            logits, loss = model(x, y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        if losses:\n",
    "            avg_loss = sum(losses) / len(losses)\n",
    "            logger.info(f\"Average loss on {filename}: {avg_loss:.4f}\")\n",
    "            return avg_loss\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logger.info(f\"Error: File {filename} not found\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error evaluating on {filename}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Evaluating on Test Sets\")\n",
    "\n",
    "test_loss_child = evaluate_on_test_file(\n",
    "    model, \"input_childSpeech_testSet.txt\", stoi, block_size, device, batch_size, eval_iters=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_baseline_loss(text, stoi):\n",
    "    \"\"\"\n",
    "    Calculate baseline loss using uniform character distribution.\n",
    "    This is the loss if the model just guessed randomly.\n",
    "    \"\"\"\n",
    "    char_counts = {}\n",
    "    total_chars = 0\n",
    "    for c in text:\n",
    "        if c in stoi:\n",
    "            char_counts[c] = char_counts.get(c, 0) + 1\n",
    "            total_chars += 1\n",
    "\n",
    "    entropy = 0\n",
    "    for count in char_counts.values():\n",
    "        prob = count / total_chars\n",
    "        entropy -= prob * math.log(prob)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "\n",
    "logger.info(\"Baseline Comparison:\")\n",
    "baseline_train = calculate_baseline_loss(text, stoi)\n",
    "logger.info(f\"Baseline loss (uniform distribution): {baseline_train:.4f}\")\n",
    "logger.info(f\"Training loss: {losses['train']:.4f}\")\n",
    "logger.info(f\"Validation loss: {losses['val']:.4f}\")\n",
    "if test_loss_child:\n",
    "    logger.info(f\"Test loss (Child Speech): {test_loss_child:.4f}\")\n",
    "if test_loss_shakespeare:\n",
    "    logger.info(f\"Test loss (Shakespeare): {test_loss_shakespeare:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "logger.info(\n",
    "    f\"Total execution time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
